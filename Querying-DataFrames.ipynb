{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spark Exercises\n",
    "\n",
    "Apache Spark&trade; allow you to use DataFrames to query large data files.\n",
    "\n",
    "## In this lesson you:\n",
    "* Create RDDs and apply transformations\n",
    "* Create DataFrames and operate over them.\n",
    "* Read data from multiple files\n",
    "* Write data to multiple source using diferent file formats.\n",
    "\n",
    "\n",
    "## Audience\n",
    "* Primary Audience: Data Engineers and Data Scientists\n",
    "* Secondary Audience: Data Analysts\n",
    "\n",
    "## Prerequisites\n",
    "* Web browser: Chrome or Firefox\n",
    "* Spark Installed\n",
    "* Git \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing RDDs and DataFrames\n",
    "\n",
    "Under the covers, DataFrames are derived from data structures known as Resilient Distributed Datasets (RDDs). RDDs and DataFrames are immutable distributed collections of data. Let's take a closer look at what some of these terms mean before we understand how they relate to DataFrames:\n",
    "\n",
    "* **Resilient**: They are fault tolerant, so if part of your operation fails, Spark  quickly recovers the lost computation.\n",
    "* **Distributed**: RDDs are distributed across networked machines known as a cluster.\n",
    "* **DataFrame**: A data structure where data is organized into named columns, like a table in a relational database, but with richer optimizations under the hood. \n",
    "\n",
    "Without the named columns and declared types provided by a schema, Spark wouldn't know how to optimize the executation of any computation. Since DataFrames have a schema, they use the Catalyst Optimizer to determine the optimal way to execute your code.\n",
    "\n",
    "DataFrames were invented because the business community uses tables in a relational database, Pandas or R DataFrames, or Excel worksheets. A Spark DataFrame is conceptually equivalent to these, with richer optimizations under the hood and the benefit of being distributed across a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with DataFrames\n",
    "\n",
    "Once created (instantiated), a DataFrame object has methods attached to it. Methods are operations one can perform on DataFrames such as filtering,\n",
    "counting, aggregating and many others.\n",
    "\n",
    "> <b>Example</b>: To create (instantiate) a DataFrame, use this syntax: `df = ...`\n",
    "\n",
    "To display the contents of the DataFrame, apply a `show` operation (method) on it using the syntax `df.show()`. \n",
    "\n",
    "The `.` indicates you are *applying a method on the object*.\n",
    "\n",
    "In working with DataFrames, it is common to chain operations together, such as: `df.select().filter().orderBy()`.  \n",
    "\n",
    "By chaining operations together, you don't need to save intermediate DataFrames into local variables (thereby avoiding the creation of extra objects).\n",
    "\n",
    "Also note that you do not have to worry about how to order operations because the optimizier determines the optimal order of execution of the operations for you.\n",
    "\n",
    "`df.select(...).orderBy(...).filter(...)`\n",
    "\n",
    "versus\n",
    "\n",
    "`df.filter(...).select(...).orderBy(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### DataFrames and SQL\n",
    "\n",
    "DataFrame syntax is more flexible than SQL syntax. Here we illustrate general usage patterns of SQL and DataFrames.\n",
    "\n",
    "Suppose we have a data set we loaded as a table called `myTable` and an equivalent DataFrame, called `df`.\n",
    "We have three fields/columns called `col_1` (numeric type), `col_2` (string type) and `col_3` (timestamp type)\n",
    "Here are basic SQL operations and their DataFrame equivalents. \n",
    "\n",
    "Notice that columns in DataFrames are referenced by `col(\"<columnName>\")`.\n",
    "\n",
    "| SQL                                         | DataFrame (Python)                    |\n",
    "| ------------------------------------------- | ------------------------------------- | \n",
    "| `SELECT col_1 FROM myTable`                 | `df.select(col(\"col_1\"))`             | \n",
    "| `DESCRIBE myTable`                          | `df.printSchema()`                    | \n",
    "| `SELECT * FROM myTable WHERE col_1 > 0`     | `df.filter(col(\"col_1\") > 0)`         | \n",
    "| `..GROUP BY col_2`                          | `..groupBy(col(\"col_2\"))`             | \n",
    "| `..ORDER BY col_2`                          | `..orderBy(col(\"col_2\"))`             | \n",
    "| `..WHERE year(col_3) > 1990`                | `..filter(year(col(\"col_3\")) > 1990)` | \n",
    "| `SELECT * FROM myTable LIMIT 10`            | `df.limit(10)`                        |\n",
    "| `display(myTable)` (text format)            | `df.show()`                           | \n",
    "| `display(myTable)` (html format)            | `display(df)`                         |\n",
    "\n",
    "**Hint:** You can also run SQL queries with the special syntax `spark.sql(\"SELECT * FROM myTable\")`\n",
    "\n",
    "In this course you see many other usages of DataFrames. It is left up to you to figure out the SQL equivalents \n",
    "(left as exercises in some cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs and DataFrames\n",
    "\n",
    "There are 3 ways to create RDDs or DataFrames\n",
    "* From a file or set of files\n",
    "* From data in memory\n",
    "* From another RDD/DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SparkSession\n",
    "In previous versions of Spark there were different entrypoints for the different APIs\n",
    "* SparkContext for Spark Core\n",
    "* SQLContext for Spark SQL\n",
    "* StreamingContext for DStreams\n",
    "* ... \n",
    "\n",
    "After Spark 2.0 the SparkSession is used as the only entry point for APIs. All functionlities of SparkContext are provided in Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick to find Spark installation\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The NBA Dataset \n",
    "This lesson uses the `2017_2018_NBA_Salaries.csv` data set, which is in CSV format.\n",
    "The NBA dataset contains following data:\n",
    "* Player Name\n",
    "* Player Country\n",
    "* Player Salary\n",
    "* Player Age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading NBA Dataset into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default CSV read method. No headers\n",
    "nba_no_headers = spark.read.csv(\"./2017_2018_NBA_Salaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema\n",
    "nba_no_headers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_headers = (spark.read\n",
    "                   .option(\"inferSchema\",\"true\")              # Option to tell Spark to infer the schema\n",
    "                   .option(\"header\",\"true\")                   # Option telling Spark that the file has a header\n",
    "                   .csv(\"./2017_2018_NBA_Salaries.csv\"))      # Option telling Spark where the file is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player: string (nullable = true)\n",
      " |-- NBA_Country: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema\n",
    "nba_headers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the method to read .CSV file spark.read.csv\n",
    "Spark provides special methods to read files in different format such as AVRO, PARQUET etc.\n",
    "spark.read.avro\n",
    "spark.read.parquet\n",
    "\n",
    "Take a look at the schema with the `printSchema` method. This tells you the field name, field type, and whether the column is nullable or not (default is true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_parquet = spark \\\n",
    "                .read \\\n",
    "                .parquet(\"2017_2018_NBA_Salaries_Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_avro = spark \\\n",
    "            .read \\\n",
    "            .format(\"com.databricks.spark.avro\") \\\n",
    "            .load(\"2017_2018_NBA_Salaries_Avro\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the DataFrame is created, view its contents by invoking the show method.\n",
    "\n",
    "By default, show() (without any parameters) prints the first 20 rows.\n",
    "\n",
    "Print the top n rows by invoking show(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------+---+\n",
      "|                _c0|        _c1|     _c2|_c3|\n",
      "+-------------------+-----------+--------+---+\n",
      "|             Player|NBA_Country|  Salary|Age|\n",
      "|            Zhou Qi|      China|  815615| 22|\n",
      "|      Zaza Pachulia|    Georgia| 3477600| 33|\n",
      "|      Zach Randolph|        USA|12307692| 36|\n",
      "|        Zach LaVine|        USA| 3202217| 22|\n",
      "|       Zach Collins|        USA| 3057240| 20|\n",
      "|       Yogi Ferrell|        USA| 1312611| 24|\n",
      "|       Xavier Silas|        USA|   74159| 30|\n",
      "|Xavier Rathan-Mayes|     Canada|   46080| 23|\n",
      "|     Xavier Munford|        USA|       0| 25|\n",
      "|    Wilson Chandler|        USA|12016854| 30|\n",
      "|  Willy Hernangomez|      Spain| 1435750| 23|\n",
      "|        Willie Reed|        USA| 1577230| 27|\n",
      "|Willie Cauley-Stein|        USA| 3704160| 24|\n",
      "|        Will Barton|        USA| 3533333| 27|\n",
      "|    Wesley Matthews|        USA|17884176| 31|\n",
      "|     Wesley Johnson|        USA| 5881260| 30|\n",
      "|      Wesley Iwundu|        USA| 1050000| 23|\n",
      "|       Wayne Selden|        USA| 1312611| 23|\n",
      "|    Wayne Ellington|        USA| 6270000| 30|\n",
      "+-------------------+-----------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----------+------+---+\n",
      "| Player|NBA_Country|Salary|Age|\n",
      "+-------+-----------+------+---+\n",
      "|Zhou Qi|      China|815615| 22|\n",
      "+-------+-----------+------+---+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+-----------+-------+---+\n",
      "|       Player|NBA_Country| Salary|Age|\n",
      "+-------------+-----------+-------+---+\n",
      "|      Zhou Qi|      China| 815615| 22|\n",
      "|Zaza Pachulia|    Georgia|3477600| 33|\n",
      "+-------------+-----------+-------+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+-----------+--------+---+\n",
      "|       Player|NBA_Country|  Salary|Age|\n",
      "+-------------+-----------+--------+---+\n",
      "|      Zhou Qi|      China|  815615| 22|\n",
      "|Zaza Pachulia|    Georgia| 3477600| 33|\n",
      "|Zach Randolph|        USA|12307692| 36|\n",
      "+-------------+-----------+--------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nba_no_headers.show()\n",
    "nba_headers.show(1)\n",
    "nba_parquet.show(2)\n",
    "nba_avro.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question:\n",
    "> According to our data, list out the players in USA\n",
    "\n",
    "Use the DataFrame `select` and `filter` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Player: string, NBA_Country: string, Salary: int, Age: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.select(\"Player\", \"NBA_Country\", \"Salary\", \"Age\").filter(col(\"NBA_Country\") == 'USA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In Functions\n",
    "\n",
    "Spark provides a number of <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>, many of which can be used directly with DataFrames.  Use these functions in the `filter` expressions to filter data and in `select` expressions to create derived columns.\n",
    "\n",
    "Examples of such functions are:\n",
    "* split()\n",
    "* trim()\n",
    "* substring()\n",
    "* collect_set()\n",
    "* regex_extrac()\n",
    "* regex_replace()\n",
    "* lpad\n",
    "\n",
    "for more visit the documentation:\n",
    "* Scala -> https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$\n",
    "* python -> https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question:\n",
    "> How many Players are there in USA?\n",
    "\n",
    "Use the DataFrame `count` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player: string (nullable = true)\n",
      " |-- NBA_Country: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.filter(col(\"NBA_Country\") == 'USA').count()\n",
    "nba_headers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question:\n",
    "> Who is the youngest player USA?\n",
    "\n",
    "Use the DataFrame `limit` or `first` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-------+---+\n",
      "|           Player|NBA_Country| Salary|Age|\n",
      "+-----------------+-----------+-------+---+\n",
      "|Terrance Ferguson|        USA|1785000| 19|\n",
      "+-----------------+-----------+-------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Terrance Ferguson'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_parquet.select(\"Player\", \"NBA_Country\", \"Salary\", \"Age\").filter(col(\"NBA_Country\") == 'USA') \\\n",
    "           .orderBy(\"Age\", ascending=True) \\\n",
    "           .limit(1) \\\n",
    "           .show()\n",
    "\n",
    "nba_parquet.select(\"Player\", \"NBA_Country\", \"Salary\", \"Age\").filter(col(\"NBA_Country\") == 'USA') \\\n",
    "           .orderBy(\"Age\", ascending=True) \\\n",
    "           .head()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question:\n",
    "> Answer the following question:\n",
    "> Who is the USA player with the highest salary?\n",
    "\n",
    "Use the DataFrame `limit` or `first` function\n",
    "\n",
    "Use the DataFrame `limit` or `first` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------+---+\n",
      "|       Player|NBA_Country|  Salary|Age|\n",
      "+-------------+-----------+--------+---+\n",
      "|Stephen Curry|        USA|34682550| 29|\n",
      "+-------------+-----------+--------+---+\n",
      "\n",
      "Stephen Curry\n"
     ]
    }
   ],
   "source": [
    "nba_parquet.select(\"Player\", \"NBA_Country\", \"Salary\", \"Age\").filter(col(\"NBA_Country\") == 'USA') \\\n",
    "           .orderBy(\"Salary\", ascending=False) \\\n",
    "           .limit(1) \\\n",
    "           .show()\n",
    "\n",
    "print(nba_parquet.select(\"Player\", \"NBA_Country\", \"Salary\", \"Age\").filter(\"NBA_Country == 'USA'\") \\\n",
    "           .orderBy(\"Salary\", ascending=False) \\\n",
    "           .head()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do the same using the `nba_no_headers` DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-------+---+\n",
      "|           _c0|_c1|    _c2|_c3|\n",
      "+--------------+---+-------+---+\n",
      "|Meyers Leonard|USA|9904495| 25|\n",
      "+--------------+---+-------+---+\n",
      "\n",
      "Meyers Leonard\n"
     ]
    }
   ],
   "source": [
    "nba_no_headers.filter(col(\"_c1\") == 'USA') \\\n",
    "           .orderBy(\"_c2\", ascending=False) \\\n",
    "           .limit(1) \\\n",
    "           .show()\n",
    "\n",
    "print(nba_no_headers.filter(col(\"_c1\") == 'USA') \\\n",
    "           .orderBy(\"_c2\", ascending=False) \\\n",
    "           .head()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something seems to be wrong with the previous result. What is it? Remember that we did not inferred the schema for this DataFrame!! \n",
    "> What would you do in order to get the highest payed player using the `nba_no_headers` DataFrame?\n",
    "\n",
    "Think on renaming the columns and casting too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------+---+\n",
      "|       Player|NBA_Country|  Salary|Age|\n",
      "+-------------+-----------+--------+---+\n",
      "|Stephen Curry|        USA|34682550| 29|\n",
      "+-------------+-----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nba_no_headers.select(col(\"_c0\").alias(\"Player\"), \\\n",
    "                      col(\"_c1\").alias(\"NBA_Country\"), \\\n",
    "                      col(\"_c2\").cast('integer').alias(\"Salary\"), \\\n",
    "                      col(\"_c3\").cast('integer').alias(\"Age\")) \\\n",
    "              .orderBy(\"Salary\", ascending=False) \\\n",
    "              .limit(1) \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we are mostly interested in USA players.\n",
    "> Can you think of performance improvement to our data processing?\n",
    "\n",
    "Think about `cache()` and `persist()` functions. What is the difference between each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Player: string, NBA_Country: string, Salary: int, Age: int]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "nba_parquet.filter(nba_parquet.NBA_Country == 'USA').cache()\n",
    "\n",
    "nba_parquet.filter(nba_parquet.NBA_Country == 'USA').persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Too much BIG DATA to fit in memory? See below\n",
    "nba_parquet.filter(nba_parquet.NBA_Country == 'USA').persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to output the NBA data into our database we need `first name` and `last name`\n",
    "> Think of a way to split `Player` column into two columns: `FirstName` and `LastName`\n",
    "\n",
    "Use `split`, `withColumn` and `drop` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_first_last_name = nba_parquet \\\n",
    "                        .withColumn(\"FirstName\", trim(split(col(\"Player\"),\" \").getItem(0))) \\\n",
    "                        .withColumn(\"LastName\", trim(split(nba_parquet.Player,\" \")[1])) \\\n",
    "                        .drop(col(\"Player\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+---------+------------+\n",
      "|NBA_Country|  Salary|Age|FirstName|    LastName|\n",
      "+-----------+--------+---+---------+------------+\n",
      "|      China|  815615| 22|     Zhou|          Qi|\n",
      "|    Georgia| 3477600| 33|     Zaza|    Pachulia|\n",
      "|        USA|12307692| 36|     Zach|    Randolph|\n",
      "|        USA| 3202217| 22|     Zach|      LaVine|\n",
      "|        USA| 3057240| 20|     Zach|     Collins|\n",
      "|        USA| 1312611| 24|     Yogi|     Ferrell|\n",
      "|        USA|   74159| 30|   Xavier|       Silas|\n",
      "|     Canada|   46080| 23|   Xavier|Rathan-Mayes|\n",
      "|        USA|       0| 25|   Xavier|     Munford|\n",
      "|        USA|12016854| 30|   Wilson|    Chandler|\n",
      "|      Spain| 1435750| 23|    Willy| Hernangomez|\n",
      "|        USA| 1577230| 27|   Willie|        Reed|\n",
      "|        USA| 3704160| 24|   Willie|Cauley-Stein|\n",
      "|        USA| 3533333| 27|     Will|      Barton|\n",
      "|        USA|17884176| 31|   Wesley|    Matthews|\n",
      "|        USA| 5881260| 30|   Wesley|     Johnson|\n",
      "|        USA| 1050000| 23|   Wesley|      Iwundu|\n",
      "|        USA| 1312611| 23|    Wayne|      Selden|\n",
      "|        USA| 6270000| 30|    Wayne|   Ellington|\n",
      "|        USA| 1874400| 21|     Wade|     Baldwin|\n",
      "+-----------+--------+---+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nba_first_last_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views\n",
    "\n",
    "In DataFrames, <b>temporary views</b> are used to make the DataFrame available to SQL, and work with SQL syntax seamlessly.\n",
    "\n",
    "A temporary view gives you a name to query from SQL, but unlike a table it exists only for the duration of your Spark Session. As a result, the temporary view will not carry over when you restart the cluster or switch to a new notebook. It also won't show up in the Data button on the menu on the left side of a Databricks notebook which provides easy access to databases and tables.\n",
    "\n",
    "The statement in the following cells create a temporary view containing the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.createOrReplaceTempView(\"nbaPlayers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the contents of temporary view, use select notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------+---+\n",
      "|             Player|NBA_Country|  Salary|Age|\n",
      "+-------------------+-----------+--------+---+\n",
      "|      Zach Randolph|        USA|12307692| 36|\n",
      "|        Zach LaVine|        USA| 3202217| 22|\n",
      "|       Zach Collins|        USA| 3057240| 20|\n",
      "|       Yogi Ferrell|        USA| 1312611| 24|\n",
      "|       Xavier Silas|        USA|   74159| 30|\n",
      "|     Xavier Munford|        USA|       0| 25|\n",
      "|    Wilson Chandler|        USA|12016854| 30|\n",
      "|        Willie Reed|        USA| 1577230| 27|\n",
      "|Willie Cauley-Stein|        USA| 3704160| 24|\n",
      "|        Will Barton|        USA| 3533333| 27|\n",
      "|    Wesley Matthews|        USA|17884176| 31|\n",
      "|     Wesley Johnson|        USA| 5881260| 30|\n",
      "|      Wesley Iwundu|        USA| 1050000| 23|\n",
      "|       Wayne Selden|        USA| 1312611| 23|\n",
      "|    Wayne Ellington|        USA| 6270000| 30|\n",
      "|       Wade Baldwin|        USA| 1874400| 21|\n",
      "|       Vince Hunter|        USA|   50000| 23|\n",
      "|       Vince Carter|        USA| 8000000| 41|\n",
      "|     Victor Oladipo|        USA|21000000| 25|\n",
      "|        Vander Blue|        USA|   50000| 25|\n",
      "+-------------------+-----------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"SELECT * FROM  nbaPlayers WHERE NBA_Country = 'USA' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     418|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) FROM  nbaPlayers WHERE NBA_Country = 'USA' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|     NBA_Country|count(1)|\n",
      "+----------------+--------+\n",
      "|          Russia|       1|\n",
      "|         Senegal|       1|\n",
      "|          Sweden|       1|\n",
      "|          Turkey|       5|\n",
      "|Democratic Re...|       2|\n",
      "|         Germany|       5|\n",
      "|          France|       9|\n",
      "|          Greece|       2|\n",
      "|       Argentina|       2|\n",
      "|         Finland|       1|\n",
      "|           China|       1|\n",
      "|         Bahamas|       1|\n",
      "|          Bosnia|       1|\n",
      "|     Puerto Rico|       2|\n",
      "|         Croatia|       6|\n",
      "|United Kingdo...|       2|\n",
      "|           Italy|       2|\n",
      "|       Lithuania|       3|\n",
      "|           Spain|       7|\n",
      "|Bosnia & Herz...|       1|\n",
      "+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"SELECT NBA_Country, COUNT(*) FROM nbaPlayers GROUP BY NBA_Country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[NBA_Country: string, count: bigint]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.groupBy(\"NBA_Country\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Player: string, NBA_Country: string, Salary: int, Player_Age: int]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.select(\"Player\", \"NBA_Country\", \"Salary\", col(\"Age\").alias(\"Player_Age\")).filter(\"NBA_Country = 'USA'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Aggregations\n",
    "\n",
    "Using built-in Spark functions</a>, you can aggregate data in various ways. \n",
    "\n",
    "Run the cell below to compute the average of all salaries in the DataFrame for USA.\n",
    "\n",
    "By default, you get a floating point value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    averageSalary|\n",
      "+-----------------+\n",
      "|5822781.588516747|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.filter(\"NBA_Country = 'USA'\") \\\n",
    "           .select(avg(\"Salary\").alias(\"averageSalary\")) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------+\n",
      "|     max|min|averageSalary|\n",
      "+--------+---+-------------+\n",
      "|34682550|  0|    5822782.0|\n",
      "+--------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nba_parquet.filter(\"NBA_Country = 'USA'\") \\\n",
    "           .select( \\\n",
    "                max(\"Salary\").alias(\"max\"), \\\n",
    "                min(\"Salary\").alias(\"min\"), \\\n",
    "                round(avg(\"Salary\")).alias(\"averageSalary\")) \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Two Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df = nba_first_last_name.filter(\"NBA_Country = 'USA'\")\n",
    "\n",
    "canada_df = nba_first_last_name.filter(\"NBA_Country = 'Canada'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_canada_df = usa_df.join(canada_df, usa_df.LastName == canada_df.LastName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---+---------+--------+-----------+--------+---+---------+--------+\n",
      "|NBA_Country|  Salary|Age|FirstName|LastName|NBA_Country|  Salary|Age|FirstName|LastName|\n",
      "+-----------+--------+---+---------+--------+-----------+--------+---+---------+--------+\n",
      "|        USA| 1471382| 24|   Norman|  Powell|     Canada| 9003125| 26|   Dwight|  Powell|\n",
      "|        USA|   86119| 29|  MarShon|  Brooks|     Canada|  815615| 22|   Dillon|  Brooks|\n",
      "|        USA|17826150| 27|     Klay|Thompson|     Canada|16400000| 26|  Tristan|Thompson|\n",
      "|        USA| 3028410| 27|    James|   Ennis|     Canada| 1524305| 23|    Tyler|   Ennis|\n",
      "|        USA| 1312611| 21| Dejounte|  Murray|     Canada| 3355320| 20|    Jamal|  Murray|\n",
      "|        USA| 2116955| 33|    Aaron|  Brooks|     Canada|  815615| 22|   Dillon|  Brooks|\n",
      "+-----------+--------+---+---------+--------+-----------+--------+---+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usa_canada_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "**Q:** What Scala keyword is used to *create* a DataFrame object?  \n",
    "**A:** An object is created by invoking the `val` keyword, so `val myDataFrameDF`. \n",
    "\n",
    "**Q:** What methods (operations) can you perform on a DataFrame object?  \n",
    "**A:** The full list is here: <a href=\"http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html\" target=\"_blank\"> pyspark.sql module</a>\n",
    "\n",
    "**Q:** Why do you chain methods (operations) `myDataFrameDF.select().filter().groupBy()`?  \n",
    "**A:** To avoid the creation of temporary DataFrames as local variables. \n",
    "\n",
    "For example, you could have written the above as \n",
    "`val tempDF1 = myDataFrameDF.select()`,  `val tempDF2 = tempDF1.filter()` and\n",
    "then `tempDF2.groupBy()`. \n",
    "\n",
    "This is syntatically equivalent, but, notice how you now have extra local variables.\n",
    "\n",
    "**Q:** What is the DataFrame syntax to create a temporary view?    \n",
    "**A:** ```myDF.createOrReplaceTempView(\"MyTempView\")```\n",
    "\n",
    "**Q:** What is the DataFrame equivalent of the SQL statement SELECT count(*) AS total\n",
    "**A:**.agg(count(\"*\").alias(\"total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Topics & Resources\n",
    "\n",
    "* <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "name": "02-Querying-Files",
  "notebookId": 343733620182772
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
