{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Querying with Dataframes\n",
    "\n",
    "Apache Spark&trade; allow you to use DataFrames to query large data files.\n",
    "\n",
    "## In this lesson you:\n",
    "* Learn about Spark DataFrames.\n",
    "* Query large files using Spark DataFrames.\n",
    "\n",
    "\n",
    "## Audience\n",
    "* Primary Audience: Data Engineers and Data Scientists\n",
    "* Secondary Audience: Data Analysts\n",
    "\n",
    "## Prerequisites\n",
    "* Web browser: Chrome or Firefox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing DataFrames\n",
    "\n",
    "Under the covers, DataFrames are derived from data structures known as Resilient Distributed Datasets (RDDs). RDDs and DataFrames are immutable distributed collections of data. Let's take a closer look at what some of these terms mean before we understand how they relate to DataFrames:\n",
    "\n",
    "* **Resilient**: They are fault tolerant, so if part of your operation fails, Spark  quickly recovers the lost computation.\n",
    "* **Distributed**: RDDs are distributed across networked machines known as a cluster.\n",
    "* **DataFrame**: A data structure where data is organized into named columns, like a table in a relational database, but with richer optimizations under the hood. \n",
    "\n",
    "Without the named columns and declared types provided by a schema, Spark wouldn't know how to optimize the executation of any computation. Since DataFrames have a schema, they use the Catalyst Optimizer to determine the optimal way to execute your code.\n",
    "\n",
    "DataFrames were invented because the business community uses tables in a relational database, Pandas or R DataFrames, or Excel worksheets. A Spark DataFrame is conceptually equivalent to these, with richer optimizations under the hood and the benefit of being distributed across a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interacting with DataFrames\n",
    "\n",
    "Once created (instantiated), a DataFrame object has methods attached to it. Methods are operations one can perform on DataFrames such as filtering,\n",
    "counting, aggregating and many others.\n",
    "\n",
    "> <b>Example</b>: To create (instantiate) a DataFrame, use this syntax: `df = ...`\n",
    "\n",
    "To display the contents of the DataFrame, apply a `show` operation (method) on it using the syntax `df.show()`. \n",
    "\n",
    "The `.` indicates you are *applying a method on the object*.\n",
    "\n",
    "In working with DataFrames, it is common to chain operations together, such as: `df.select().filter().orderBy()`.  \n",
    "\n",
    "By chaining operations together, you don't need to save intermediate DataFrames into local variables (thereby avoiding the creation of extra objects).\n",
    "\n",
    "Also note that you do not have to worry about how to order operations because the optimizier determines the optimal order of execution of the operations for you.\n",
    "\n",
    "`df.select(...).orderBy(...).filter(...)`\n",
    "\n",
    "versus\n",
    "\n",
    "`df.filter(...).select(...).orderBy(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### DataFrames and SQL\n",
    "\n",
    "DataFrame syntax is more flexible than SQL syntax. Here we illustrate general usage patterns of SQL and DataFrames.\n",
    "\n",
    "Suppose we have a data set we loaded as a table called `myTable` and an equivalent DataFrame, called `df`.\n",
    "We have three fields/columns called `col_1` (numeric type), `col_2` (string type) and `col_3` (timestamp type)\n",
    "Here are basic SQL operations and their DataFrame equivalents. \n",
    "\n",
    "Notice that columns in DataFrames are referenced by `col(\"<columnName>\")`.\n",
    "\n",
    "| SQL                                         | DataFrame (Python)                    |\n",
    "| ------------------------------------------- | ------------------------------------- | \n",
    "| `SELECT col_1 FROM myTable`                 | `df.select(col(\"col_1\"))`             | \n",
    "| `DESCRIBE myTable`                          | `df.printSchema()`                    | \n",
    "| `SELECT * FROM myTable WHERE col_1 > 0`     | `df.filter(col(\"col_1\") > 0)`         | \n",
    "| `..GROUP BY col_2`                          | `..groupBy(col(\"col_2\"))`             | \n",
    "| `..ORDER BY col_2`                          | `..orderBy(col(\"col_2\"))`             | \n",
    "| `..WHERE year(col_3) > 1990`                | `..filter(year(col(\"col_3\")) > 1990)` | \n",
    "| `SELECT * FROM myTable LIMIT 10`            | `df.limit(10)`                        |\n",
    "| `display(myTable)` (text format)            | `df.show()`                           | \n",
    "| `display(myTable)` (html format)            | `display(df)`                         |\n",
    "\n",
    "**Hint:** You can also run SQL queries with the special syntax `spark.sql(\"SELECT * FROM myTable\")`\n",
    "\n",
    "In this course you see many other usages of DataFrames. It is left up to you to figure out the SQL equivalents \n",
    "(left as exercises in some cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Data \n",
    "This lesson uses the `2017_2018_NBA_Salaries.csv` data set, which is in CSV format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "nbaDF = spark.read.csv(\"C:\\Users\\<userName>\\Desktop\\demoNotes\\2017_2018_NBA_Salaries.csv\")\n",
    "\n",
    "\n",
    "# CSV files read \n",
    "nbaDF = (spark\n",
    "  .read                                                # Call the read method returning a DataFrame\n",
    "  .option(\"inferSchema\",\"true\")                        # Option to tell Spark to infer the schema\n",
    "  .option(\"header\",\"true\")                             # Option telling Spark that the file has a header\n",
    "  .csv(\"C:\\Users\\<userName>\\Desktop\\demoNotes\\2017_2018_NBA_Salaries.csv\"))  # Option telling Spark where the file is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the method to read .CSV file spark.read.csv\n",
    "Spark provides special methods to read files in different format such as AVRO, PARQUET etc.\n",
    "spark.read.avro\n",
    "spark.read.parquet\n",
    "\n",
    "Take a look at the schema with the `printSchema` method. This tells you the field name, field type, and whether the column is nullable or not (default is true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbaDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the DataFrame is created, view its contents by invoking the show method.\n",
    "\n",
    "By default, show() (without any parameters) prints the first 20 rows.\n",
    "\n",
    "Print the top n rows by invoking show(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbaDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question:\n",
    "> According to our data, list out the players in USA\n",
    "\n",
    "Use the DataFrame `select` and `filter` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.select(\"Palyer\", \"NBA_Country\", \"Salary\", \"Age\").filter(\"NBA_Country = 'USA'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In Functions\n",
    "\n",
    "Spark provides a number of <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>, many of which can be used directly with DataFrames.  Use these functions in the `filter` expressions to filter data and in `select` expressions to create derived columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many Players are there in USA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.filter(\"NBA_Country = 'USA'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is the youngest player USA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.select(\"Palyer\", \"NBA_Country\", \"Salary\", \"Age\").filter(\"NBA_Country = 'USA'\").orderBy(\"Age\").limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views\n",
    "\n",
    "In DataFrames, <b>temporary views</b> are used to make the DataFrame available to SQL, and work with SQL syntax seamlessly.\n",
    "\n",
    "A temporary view gives you a name to query from SQL, but unlike a table it exists only for the duration of your Spark Session. As a result, the temporary view will not carry over when you restart the cluster or switch to a new notebook. It also won't show up in the Data button on the menu on the left side of a Databricks notebook which provides easy access to databases and tables.\n",
    "\n",
    "The statement in the following cells create a temporary view containing the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.createOrReplaceTempView(\"nbaPlayers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the contents of temporary view, use select notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"SELECT * FROM  nbaPlayers WHERE NBA_Country = 'USA' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"SELECT COUNT(1) FROM  nbaPlayers WHERE NBA_Country = 'USA' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.sql(\"SELECT NBA_Country, COUNT(1) FROM nbaPlayers GROUP BY NBA_Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.groupBy(\"NBA_Country\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.select(\"Palyer\", \"NBA_Country\", \"Salary\", col(\"Age\").alias(\"Player_Age\")).filter(\"NBA_Country = 'USA'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Aggregations\n",
    "\n",
    "Using built-in Spark functions</a>, you can aggregate data in various ways. \n",
    "\n",
    "Run the cell below to compute the average of all salaries in the DataFrame for USA.\n",
    "\n",
    "By default, you get a floating point value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF.select(avg(\"Salary\").alias(\"averageSalary\")).filter(\"NBA_Country = 'USA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nbaDF..select(max(\"Salary\").alias(\"max\"), min(\"Salary\").alias(\"min\"), round(avg(\"Salary\")).alias(\"averageSalary\")).filter(\"NBA_Country = 'USA'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "**Q:** What Scala keyword is used to *create* a DataFrame object?  \n",
    "**A:** An object is created by invoking the `val` keyword, so `val myDataFrameDF`. \n",
    "\n",
    "**Q:** What methods (operations) can you perform on a DataFrame object?  \n",
    "**A:** The full list is here: <a href=\"http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html\" target=\"_blank\"> pyspark.sql module</a>\n",
    "\n",
    "**Q:** Why do you chain methods (operations) `myDataFrameDF.select().filter().groupBy()`?  \n",
    "**A:** To avoid the creation of temporary DataFrames as local variables. \n",
    "\n",
    "For example, you could have written the above as \n",
    "`val tempDF1 = myDataFrameDF.select()`,  `val tempDF2 = tempDF1.filter()` and\n",
    "then `tempDF2.groupBy()`. \n",
    "\n",
    "This is syntatically equivalent, but, notice how you now have extra local variables.\n",
    "\n",
    "**Q:** What is the DataFrame syntax to create a temporary view?    \n",
    "**A:** ```myDF.createOrReplaceTempView(\"MyTempView\")```\n",
    "\n",
    "**Q:** What is the DataFrame equivalent of the SQL statement SELECT count(*) AS total\n",
    "**A:**.agg(count(\"*\").alias(\"total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Topics & Resources\n",
    "\n",
    "* <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "name": "02-Querying-Files",
  "notebookId": 343733620182772
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
